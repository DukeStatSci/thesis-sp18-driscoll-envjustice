# Approaches and Methods {#ref-labels}

## Geographic Level of Analysis

The questions we would like to pose - how the distributions of toxicity that individuals experience over time are predicted by their complex, multidimensional identities - is inherently intended to use individuals as the unit of analysis. Until now environmental justice work has used census geographies or point sources as the unit of analysis. 

We depend on two data sources, the disaggregated RSEI toxic release data (as compiled to contain only releases that are consistently reported between 1990 and 2010) as well as relevant demographic information from the Census. 

RSEI toxicity data can be obtained at extremely fine level (the 800 meter grid across the United States,) but the finest grain Census data is available at is the block level, which contains between 0 and a few thousand people. At such low geographic levels, cross tabs aren't available for demographics due to identifiability concerns. Using a low level of geography (like census blocks or block groups) is important for the environmental aspect of this analysis, since environmental hazards can be very localized, especially along neighborhood lines in urban areas. 

Unfortunately, the availability of cross tabulations is equally important to the goal of this work in examining inequality of environmental burden held by minority groups in America. The intersection of social identities, especially those steeped in systems of oppression, is extremely important for identifying unequal burdens. For example, low income populations across the board may be more likely to experience environmental hazards, but low income minority populations may be much more likely than low income white populations to experience extreme hazard. The intersections of demographic characteristics, such as race and income or race and education are likely to be important in teasing out differences true inequality burden. 

We combine the computed aggregated toxicity for each block group and the demographic data. Now for each census geography, we have toxicity information as well as demographic data. 

|block       |concentration|area    |total_pop|white|black|
|------------|-------------|--------|---------|-----|-----|
|010010201001|627.3050     |6.520168|530      |447  |83   |
|010010201002|499.6298     |8.486690|1282     |1099 |126  |
|010010202001|578.8312     |3.137173|1274     |363  |824  |
|010010202002|756.3733     |1.962949|944      |458  |477  |
|010010203001|637.7356     |5.907125|2538     |2152 |384  |
|...         |...          |...     |...      |...  |...  |
To move to an individual level analysis, we assign each person the toxicity for the geography they originated in. By assigning each person the toxicity of their block group, we can aggregate nationally to find the distribution of toxicity that each social group experiences. This approach is restricted by Census data availability, since we can only build a distribution for each of the crosstabs we have available. For higher levels of geography (where we might, for example, have race by income) we would be able to build national distributions for each income by race group.

In the case of the table above, to build a distribution for the white population, we would assign 447 people a toxicity of ~627, 1099 people a toxicity of ~499 and so on until we have the full distribution of toxicities experienced by the white population.

In choosing the level of aggregation at which to assign toxicity, in order to balance the needs of accuracy of toxicity and availability of cross tabulations, we create the overall toxicity distribution for Americans at each of the levels of geography. The process described above can be executed with the data shown above, or at a cruder level of geography, such as state or region. Using block group as the smallest form of geography, and state as the largest we build toxicity distributions at each level of aggregation. 

```{r ecologicalFallacy, echo = FALSE, fig.width=10, fig.height=5, message = FALSE}
#import toxicity data
bg2010 = as.data.frame(fread("data/toxic/bg/toxic_2010_2010_blockgroup.csv"))

#import census data
census_bg = fread("data/ecologicalFallacy/census_bg_race.csv", skip = 1)
census_tract = fread("data/ecologicalFallacy/census_tract_race.csv")
census_county = fread("data/ecologicalFallacy/census_county_race.csv", skip = 1)
census_state = fread("data/ecologicalFallacy/census_state_race.csv", skip = 1)

#munge census data
census_bg = select(census_bg, Geo_FIPS, SE_T013_001, SE_T013_002, SE_T013_003)
names(census_bg)  = c("fips", "total", "white", "black")
census_bg$fips = str_pad(census_bg$fips, 12, pad = "0", "left")
census_tract = select(census_tract, Geo_FIPS, SE_T013_001, SE_T013_002, SE_T013_003)
names(census_tract)  = c("fips", "total", "white", "black")
census_county = select(census_county, Geo_FIPS, SE_T013_001, SE_T013_002, SE_T013_003)
names(census_county)  = c("fips", "total", "white", "black")
census_county$fips = str_pad(census_county$fips, 5, pad = "0", side = "left")
census_state = select(census_state, Geo_FIPS, SE_T013_001, SE_T013_002, SE_T013_003)
names(census_state)  = c("fips", "total", "white", "black")

#merge census and toxicity data
tract2010 = fread("data/ecologicalFallacy/toxic_2010_2010_tract.csv")
tract2010 = merge(tract2010, census_tract, by.x = "block", by.y = "fips")
county2010 = fread("data/ecologicalFallacy/toxic_2010_2010_county.csv")
county2010 = merge(county2010, census_county, by.x = "block", by.y = "fips")
state2010 = fread("data/ecologicalFallacy/toxic_2010_2010_state.csv", data.table = FALSE)
state2010 = merge(state2010, census_state, by.x = "block", by.y = "fips")
bg2010 = merge(bg2010, census_bg, by.x = "block", by.y = "fips")

#plot national distributions
g = ggplot() + geom_density(aes(concentration, weight = white/sum(white), color = "County"), bw = 0.08, data = county2010)
g = g + geom_density(aes(concentration, weight = white/sum(white), color = "State"), bw = .1, data = state2010)
g = g + geom_density(aes(concentration, weight = white/sum(white), color = "Block Group"), bw = 0.08, size = 1, data = bg2010)
g = g + geom_density(aes(concentration, weight = white/sum(white), color = "Tract"), bw = 0.05, size = 1, data = tract2010)
g = g + labs(x = "Log Toxicity", colour = "Geographic Level", caption = "(as assigned by different levels of geography)", title = "Distribution of Toxicity for US Population")
g = g + theme(axis.text=element_text(size = 6), legend.text = element_text(size = 7), legend.title = element_text(size = 8))
g = g + scale_x_log10(breaks = scales::trans_breaks("log10", function(x) 10^x),
                          labels = scales::trans_format("log10", scales::math_format(10^.x))) +
      annotation_logticks(
        short = unit(.5,"mm"),
        mid = unit(2,"mm"), 
        long = unit(3.5,"mm"),
        sides = "b",
        color = "gray65") 
g
```

As expected, the state level assignment is a poor approximation of the lower level assignments. Given that we are assigning each individual the mean toxicity in their entire state, we are eliminating most of the variation from the data. Interestingly tract and county data seems to build a distribution quite similar to the block group level assignment. Initial results were replicated using all 3 levels of assignment, and conclusions remained the same. This may be because the block group level is aggregating a large enough group of our fine grain toxicity data that it has already lost the street block by street block variation that we had deemed so crucial, meaning aggregating several block groups gives us a conceptually equivalent 'neighborhood' level of aggregation. 

The ecological fallacy is a constant discussion in environmental justice literature. It is speculated to be the reason for varying results, as toxicity data can be sensitive to geographic level of analysis, time of collection, and area. In this case, conclusions appear to be relatively robust to unit of assignments.


## Simulation
```{r densityCalcs, include = FALSE}
density_black = ewcdf(bg2010$concentration, weights = bg2010$black/sum(bg2010$black))
black2010 = round(quantile(density_black, 0.75), 2)
density_overall = ewcdf(bg2010$concentration, weights = bg2010$total/sum(bg2010$total))
overallperc = round(density_overall(black2010), 4)*100
```

To examine how environmental burden changes over time for minority groups we use simulation to tease apart the forces at play in each group's changing distributions. We expect the mean of minority distributions to reduce over time for two reasons: the toxicity distribution for the entire population is slowly shifting right and compressing as we see improvements in environmentally friendly production technology and more comprehensive environmental regulation; secondly, we hope that with Title VI protections and the work of civil rights advocates, minority communities will be better equipped to mobilize against polluters, shifting the mean of minority distribution right relative to the overall distribution.

These two explanations for decreasing toxicity can translate into two descriptors of distributional change:
 
  * distributional convergence: where inequality is reduced due to compression of the overall distribution, disproprortionately improving toxicity for those at the right tail. 
  
  * positional convergence: where inequality is reduced due to shifting distributions, reducing the difference between means of each distribution.
  
Positional convergence is of primary interest, since it would allow us to look at how much 'true' change there has been. By removing the distributional convergence we are able to compare the current observed reductions in inequality to what those changes would have been if minority populations had held static their position in the overall distribution.

The percentile any given toxicity value holds in a minority distribution is different from the percentile it holds in the overall distribution. For example, in 2010 the 75th percentile of the black distribution was `r black2010`, while that same value was the `r overallperc`th percentile in the overall distribution. 

In order to find the positional convergence of minority distributions over the period of study, we use the percentiles that minority individuals held in the overall distribution at the start of the period of study and propogate them forward to simulate how each group's distribution would have progressed through time. This simulation procedure was first discussed by Bayer, Patrick and Kerwin as a method of identifying the true gains in 'Black-White Earning Differences' (2016). 

This simulation proceeds as follows:

* Build an empirical distribution of toxicity experienced for the entire population and for each group of interest in the starting year. 

* Sample individuals from the empirical distributions of the groups of interest. 

* For each sampled toxicity value, find the percentile it holds in the empirical distribution for the entire population in the starting year. 

* Create an empirical distribution for the entire population in the ending year. 

* For each computed percentile, find the corresponding toxicity value in the full empirical distribution of the ending year and compile results to create a simulated group of interest.

Using this method we can hold constant the place each individual (and more broadly each group) holds in the overall distribution, but follow the changes in the distribution as a whole. The collection of values simulated now represents where each individual or group would have been if they had held the same relative position in the overall toxicity distribution, meaning there had been no positional improvement for the group as a whole.

If there had been positional improvement for a group, we would expect the simulated distributions to paint a bleaker picture of the inequalities of environmental burden borne than the observed distribution of the ending year. 

###Simulation Accuracy

We use bootstrap sampling to approximate the standard errors of the simulated estimates of the 5th, 50th and 95th quantiles with different sampling sizes. This is done by simulating the new distribution at time t repeatedly with varying n, and describing the distribution of quantiles produced. This sampling helps us to choose a sample size that gives a sufficiently precise estimation, and frames the final values we report. 

```{r simulationSE, echo = FALSE, warning = FALSE, message = FALSE, fig.width=8, fig.height=3}
sim_se_05 = read_csv("data/bootstrap_simulation_se_05.csv")
sim_se_50 = read_csv("data/bootstrap_simulation_se_50.csv")
sim_se_95 = read_csv("data/bootstrap_simulation_se_95.csv")

s5 = ggplot() + 
  geom_ribbon(aes(x = n, ymin = mean-sd, ymax = mean+sd), alpha=0.2, data = sim_se_05) +
  geom_line(aes(x = n, y = mean), data = sim_se_05) +
  labs(x = "Size of simulated group", y = "Simulated 5th Percentile") + coord_cartesian(ylim=c(40, 100))

s50 = ggplot() + 
  geom_ribbon(aes(x = n, ymin = mean-sd, ymax = mean+sd), alpha=0.2, data = sim_se_50) +
  geom_line(aes(x = n, y = mean), data = sim_se_50) +
  labs(x = "Size of simulated group", y = "Simulated 50th Percentile") + coord_cartesian(ylim=c(7000, 9000))

s95 = ggplot() + 
  geom_ribbon(aes(x = n, ymin = mean-sd, ymax = mean+sd), alpha=0.2, data = sim_se_95) +
  geom_line(aes(x = n, y = mean), data = sim_se_95) +
  labs(x = "Size of simulated group", y = "Simulated 95th Percentile") + coord_cartesian(ylim=c(80000, 140000))

grid.arrange(s5, s50, s95, ncol=3)

```

For the 5th quantile, a relatively small sample produces a fairly stable result, sampling with n = 15,000 appears sufficient. Due to the extreme right skew of the data, the 95th percentile requires a larger sample. Still n = 50,000 is sufficient.
